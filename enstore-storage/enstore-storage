#!/usr/bin/python

import time

from gratia.common.Gratia import DebugPrint
#import gratia.common.GratiaWrapper as GratiaWrapper
import gratia.common.Gratia as Gratia
from gratia.services.StorageElement import StorageElement
from gratia.services.StorageElementRecord import StorageElementRecord

from gratia.common2.meter import GratiaMeter
from gratia.common2.pgpinput import PgInput
from gratia.common2.checkpoint import DateTransactionAuxCheckpoint
import gratia.common2.timeutil as timeutil


def DebugPrintLevel(level, *args):
    if level <= 0:
        level_str = "CRITICAL"
    elif level >= 4:
        level_str = "DEBUG"
    else:
        level_str = ["CRITICAL", "ERROR", "WARNING", "INFO", "DEBUG"][level]
    level_str = "%s - EnstoreStorage: " % level_str
    DebugPrint(level, level_str, *args)


class _EnstoreStorageInputStub:
    """Stub class, needs to be defined before the regular one, to avoid NameError
    """
    value_matrix = [['2013-05-01 00:00:00', 'ALEX',              27501971200,    29502252800,           94,         101],
['2013-05-01 00:00:00', 'AMN',                         0,              0,            0,           0],
['2013-05-01 00:00:00', 'ANM',             5324442004458,  5489039007858,         1645,        2087],
['2013-05-01 00:00:00', 'none',                        0,              0,            0,           0],
['2013-06-01 00:00:00', 'ALEX',              32502444800,    34502726400,          111,         118],
['2013-06-01 00:00:00', 'AMN',                         0,              0,            0,           0],
['2013-06-01 00:00:00', 'ANM',             5330140154858,  5494737158258,         1654,        2096],
['2013-06-01 00:00:00', 'none',                        0,              0,            0,           0],
['2013-07-01 00:00:00', 'ALEX',              32702534400,    34702816000,          112,         119],
['2013-07-01 00:00:00', 'AMN',                         0,              0,            0,           0],
['2013-07-01 00:00:00', 'ANM',             5363559205866,  5533639729266,         1745,        2190],
['2013-07-01 00:00:00', 'none',                        0,              0,            0,           0],
['2013-08-01 00:00:00', 'ANM',             9715492242387,  9894136393825,         7715,        8179],
['2013-08-01 00:00:00', 'none',                        0,              0,            0,           0],
['2014-02-01 00:00:00', 'ANM',             9999003890760, 10599915674909,         7964,        8601],
['2014-02-01 00:00:00', 'litvinse',            231480377,      231480377,          120,         120],
['2014-02-01 00:00:00', 'test',                  2100468,        4200936,            3,           6],
['2014-03-01 00:00:00', 'ALEX',              10400563200,    10400563200,           62,          62],
['2014-03-01 00:00:00', 'ANM',             9343845119048, 10192509713697,         7914,        8621],
['2014-03-01 00:00:00', 'litvinse',            231480377,      231480377,          120,         120],
['2014-03-01 00:00:00', 'none',                        0,              0,            0,           0],
['2014-03-01 00:00:00', 'test',                  2100468,        4200936,            3,           6],
['2014-04-01 00:00:00', 'ALEX',              10400563200,    10400563200,           62,          62],
['2014-04-01 00:00:00', 'ANM',             9430388985014, 10280528687817,         8222,        9025],
['2014-04-01 00:00:00', 'e906',                        0,              0,            0,           0],
['2014-04-01 00:00:00', 'litvinse',            231480377,      231480377,          120,         120],
['2014-04-01 00:00:00', 'none',                        0,              0,            0,           0],
['2014-04-01 00:00:00', 'test',                  2100468,        4200936,            3,           6],
['2014-05-01 00:00:00', 'ALEX',              15002131200,    15002131200,           54,          54],
['2014-05-01 00:00:00', 'ANM',             9433767656334,  9654836052415,         8108,        8965],
['2014-05-01 00:00:00', 'e906',                        0,              0,            0,           0],
['2014-05-01 00:00:00', 'litvinse',            231480377,      231480377,          120,         120],
['2014-05-01 00:00:00', 'test',                  2100468,        4200936,            3,           6],
['2014-06-01 00:00:00', 'ALEX',               2900518400,     2900518400,           11,          11],
['2014-06-01 00:00:00', 'ANM',             9434587326624,  9655655722705,         8145,        9002],
['2014-06-01 00:00:00', 'e906',                        0,              0,            0,           0],
['2014-06-01 00:00:00', 'litvinse',            231480377,      231480377,          120,         120],
['2014-07-01 00:00:00', 'ALEX',             910905676800,   910905676800,         3044,        3044],
['2014-07-01 00:00:00', 'ANM',            10344072760622, 10565385542446,         8877,        9846],
['2014-07-01 00:00:00', 'e906',                        0,              0,            0,           0],
['2014-07-01 00:00:00', 'litvinse',            231480377,     1102439795,          120,      377322]
    ]

    def get_records():
        for i in _EnstoreStorageInputStub.value_matrix:
            retv = {'date': timeutil.parse_datetime(i[0]),
                    'storage_group': i[1],
                    'active_bytes': long(i[2]),
                    'total_bytes': long(i[3]),
                    'active_files': long(i[4]),
                    'total_files': long(i[5])
                    }
            yield retv
    get_records = staticmethod(get_records)


class EnstoreStorageInput(PgInput):
    """Query the records form the Enstore enstoredb DB
    """

    VERSION_ATTRIBUTE = 'EnstoreVersion'

    def get_init_params(self):
        """Return list of parameters to read form the config file"""
        return PgInput.get_init_params(self) + [EnstoreStorageInput.VERSION_ATTRIBUTE]

    def add_checkpoint(self, fname=None, max_val=None, default_val=None, fullname=False):
        """Add a checkpoint, default file name is cfp-INPUT_NAME
        :param fname: checkpoint file name (considered as prefix unless fullname=True)
                file name is fname-INPUT_NAME
        :param max_val: trim value for the checkpoint
        :param default_val: value if no checkpoint is available
        :param fullname: Default: False, if true, fname is considered the full file name
        :return:
        """

        if not fname:
            fname = "cpf-%s" % self.get_name()
        else:
            if not fullname:
                fname = "%s-%s" % (fname, self.get_name())
        if max_val is not None or default_val is not None:
            self.checkpoint = DateTransactionAuxCheckpoint(fname, max_val, default_val)
        else:
            self.checkpoint = DateTransactionAuxCheckpoint(fname)

    def start(self, static_info):
        PgInput.start(self, static_info)
        DebugPrint(4, "ESI start, static info: %s" % static_info)
        if EnstoreStorageInput.VERSION_ATTRIBUTE in static_info:
            self._set_version_config(static_info[EnstoreStorageInput.VERSION_ATTRIBUTE])

    def _start_stub(self, static_info):
        """start replacement for testing: database connection errors are trapped"""
        try:
            DebugPrintLevel(4, "Testing DB connection. The probe will not use it")
            PgInput.start(self, static_info)
            if self.status_ok():
                DebugPrintLevel(4, "Connection successful")
            else:
                DebugPrintLevel(4, "Connection failed")
            DebugPrintLevel(4, "Closing the connection")
            self.stop()
        except:
            DebugPrint(1, "Database connection failed. The test can continue since stubs are used.")
        DebugPrint(4, "ESI start stub, static info: %s" % static_info)
        if EnstoreStorageInput.VERSION_ATTRIBUTE in static_info:
            self._set_version_config(static_info[EnstoreStorageInput.VERSION_ATTRIBUTE])

    def get_version(self):
        # RPM package is 'enstore'
        return self._get_version('enstore')

    @staticmethod
    def get_record_id(srecord):
        # This record should be unique. Only one storage_group per timestamp
        #TODO: verify actual DB duplicate constraint
        return "%s-%s" % (srecord['date'], srecord['storage_group'])

    def get_records(self, limit=None):
        """Select the usage records from the storage table
        enstoredb=> \d historic_tape_bytes;
          Table "public.historic_tape_bytes"
   Column     |            Type             | Modifiers
---------------+-----------------------------+-----------
date          | timestamp without time zone | not null
storage_group | character varying           | not null
active_bytes  | bigint                      |
unknown_bytes | bigint                      |
deleted_bytes | bigint                      |
active_files  | bigint                      |
unknown_files | bigint                      |
deleted_files | bigint                      |
Indexes:
   "historic_tape_bytes_pkeys" PRIMARY KEY, btree (date, storage_group)


Table is updated monthly and for each storage group contains
record about number of bytes.

We are interested in

active_bytes and (active_bytes+deleted_bytes+unknown_bytes) as total_bytes
active_files and (active_files+deleted_files+unknown_files) as total_files

NOTE The time in date is stored as local time! (Dmitry 2/1/2015)
The DB returns time (date) as naive datetime and requires strings like 'YYYY-MM-DD HH:MM:SS'
date cannot be null
        """
        checkpoint = self.checkpoint
        checkpoint_datetime = None
        ok_to_send = True
        if checkpoint:
            checkpoint_datetime = checkpoint.date()
            checkpoint_record_id = checkpoint.aux()
            # the checkpoint stores date and record_id
            # If there is a record_id is OK to send if time>cp_time or time==cp_time and
            # the last record has been encountered.
            # assuming that the order of the list returned is the same
            if checkpoint_record_id is not None:
                ok_to_send = False
            checkpoint_sql = "WHERE date >= '%s'" % timeutil.format_datetime(checkpoint_datetime, iso8601=False)
        else:
            checkpoint_sql = ""
        if limit:
            limit_sql = "LIMIT %s" % limit
        else:
            limit_sql = ""

        sql = '''SELECT
            date,
            storage_group, active_bytes,
            (active_bytes+unknown_bytes+deleted_bytes) as total_bytes,
            active_files,
            (active_files+unknown_files+deleted_files) as total_files
            FROM historic_tape_bytes
            %s
            ORDER BY date, storage_group
            %s
            ''' % (checkpoint_sql, limit_sql)

        query_start_time = time.time()
        DebugPrint(4, "Requesting new Enstore records %s" % sql)
        new_checkpoint = None
        new_checkpoint_record_id = None
        for r in self.query(sql):
            # skip records before the one checkpointed
            if not ok_to_send:
                if r['date'] > checkpoint_datetime:
                    # send also the current record
                    ok_to_send = True
                elif self.get_record_id(r) == checkpoint_record_id:
                    # send starting w/ the next record
                    ok_to_send = True
                    continue
                else:
                    continue
            # Add handy data to job record
            #r['cluster'] = self._cluster
            #self._addUserInfoIfMissing(r)
            #TODO: filter out invalid records (e.g. check for None values)?
            yield r
            if checkpoint:
                # psycopg2 returns datetime obj: timestamp->datetime, timestamp without time zone (naive)
                # checkpoint will use naive timestamps, probe will convert later to UTC (enstore uses local time)
                new_date = r['date']
                if new_checkpoint is None or new_date > new_checkpoint:
                    new_checkpoint = new_date
                    new_checkpoint_record_id = self.get_record_id(r)
        if new_checkpoint:
            ## Using last value (new_checkpoint_record_id)
            ##  Increment one second to avoid the last records matched in the next call.
            ## The increment happens only if the checkpoint is in the past. If it is at the edge of
            ## the time interval of the query (query_start_time), then there is no increment,
            ## not to lose values that could be added with the same timestamp. Here duplicates are the lesser evil.
            #new_checkpoint = timeutil.conditional_increment(new_checkpoint, query_start_time)
            #print "****** MMDB Saving New Checkpoint: %s, %s, %s" % (type(new_checkpoint),
            # new_checkpoint, datetime.datetime.fromtimestamp(new_checkpoint))
            DebugPrint(4, "Saving new Enstore checkpoint %s" % new_checkpoint)
            checkpoint.set_date_transaction_aux(new_checkpoint, 0, new_checkpoint_record_id)

    def _get_records_stub(self, limit=None):
        """get_records replacement for tests: records are from a pre-filled array"""
        checkpoint = self.checkpoint
        if checkpoint:
            DebugPrint(4, "Ignoring checkpoint in stub function: %s" % checkpoint.date())
        if limit is None:
            limit = 0
        for i in _EnstoreStorageInputStub.get_records():
            yield i
            if limit > 0:
                limit -= 1
                if limit == 0:
                    break
        if checkpoint:
            DebugPrint(4, "Stub function, saving checkpoint with same date: %s" % checkpoint.date())
            checkpoint.set_date_transaction(checkpoint.date())

    def do_test(self, static_info=None):
        """Test with pre-arranged DB query results
        """
        # replace DB calls with stubs
        self.start = self._start_stub
        self.get_records = self._get_records_stub


class EnstoreStorageProbe(GratiaMeter):

    PROBE_NAME = 'enstore-storage'
    # dCache, xrootd, Enstore
    SE_NAME = 'Enstore'
    # Production
    SE_STATUS = 'Production'
    # disk, tape
    SE_TYPE = 'tape'
    # raw, logical
    SE_MEASUREMENT_TYPE = 'logical'

    def __init__(self):
        GratiaMeter.__init__(self, self.PROBE_NAME)
        self._probeinput = EnstoreStorageInput()

    def get_storage_element(self, unique_id, site, name, parent_id=None, timestamp=None):
        """Return a StorageElement record: name and VO info. Timestamp is set only if provided"""
        if not parent_id:
            parent_id = unique_id
        gse = StorageElement()
        gse.UniqueID(unique_id)
        gse.SE(site)
        gse.Name(name)
        gse.ParentID(parent_id)
        # VO
        # OwnerDN
        gse.SpaceType("StorageGroup")  # PoolGroup, StorageGroup in enstore terminology
        if timestamp:
            gse.Timestamp(timestamp)
        gse.Implementation(self.SE_NAME)
        gse.Version(self.get_version())
        gse.Status(self.SE_STATUS)
        # ProbeName
        # probeid
        # SiteName
        # Grid
        return gse

    def get_storage_element_record(self, unique_id, timestamp=None):
        """Return a StorageElementRecord obj: id and space used. Timestamp is set only if provided"""
        gser = StorageElementRecord()
        gser.UniqueID(unique_id)
        gser.MeasurementType(self.SE_MEASUREMENT_TYPE)
        gser.StorageType(self.SE_TYPE)
        # StorageType
        if timestamp:
            gser.Timestamp(timestamp)
        # TotalSpace, Free, Used
        # FileCountLimit
        # FileCount
        # Probename
        # probeid
        return gser

    def input_to_gsrs(self, inrecord, selement, serecord):
        """(EnstoreStorage)Input to Gratia Storage RecordS.
        Add input values to storage element and storage element record
        Return the tuple VO,tot,free,used,f_tot,f_used to allow cumulative counters
        :param inrecord: EnstoreStorageInput object (dictionary)
        :param selement: StorageElement record
        :param serecord: StorageElementRecord record
        :return: Return the tuple (VO,tot,free,used,f_tot,f_used) to allow cumulative counters
        """
        # SE
        selement.VO(inrecord['storage_group'])
        selement.Name(inrecord['storage_group'])
        timestamp = timeutil.format_datetime(timeutil.datetime_to_utc(inrecord['date']))
        selement.Timestamp(timestamp)
        # SER
        # TODO: investigate different DB structure
        # Record is currently "abused"
        # used - active bytes (valid files)
        # total - active + deleted + unknown (total files on tape, not total space available)
        # free - deleted + unknown (lost, ...) files
        serecord.Timestamp(timestamp)
        used = inrecord['active_bytes']
        total = inrecord['total_bytes']
        # Values are in bytes
        serecord.TotalSpace(total)
        if total is None or used is None:
            unavailable = None
        else:
            unavailable = total - used
        serecord.FreeSpace(unavailable)
        serecord.UsedSpace(used)
        # Add file counts
        # different form total_files - serecord.FileCountLimit()
        serecord.FileCount(inrecord['active_files'])
        return inrecord['storage_group'], total, unavailable, used, 0, inrecord['active_files']

    def main(self):
        # Initialize the probe and the input
        self.start()
        DebugPrintLevel(4, "Enstore storage probe started")

        # Must be in configuration file, set initially to EnstoreStorage
        se = self.get_sitename()

        # Prepare data for parent storage element
        # Understand the meaning of the name: name = self.get_probename()
        parent_name = se
        unique_id = "%s:SE:%s" % (se, se)
        parent_id = unique_id
        # The parent record will be sent in the loop and after (one for each date where there are other records)

        srecord_date = None
        # Loop over storage records
        for srecord in self._probeinput.get_records():
            if srecord_date is None:
                srecord_date = srecord['date']
            else:
                # records are grouped and ordered by date. If date changes, will not go back to old value
                # parent record for that timestamp can be sent
                # TODO: add also a SER with summary for used and "available" space?
                if not srecord_date == srecord['date']:
                    # Parent storage element
                    DebugPrintLevel(4, "Sending the parent StorageElement (%s/%s/%s)" % (se, parent_name, srecord_date))
                    timestamp = timeutil.format_datetime(timeutil.datetime_to_utc(srecord_date))
                    gse = self.get_storage_element(parent_id, se, parent_name, timestamp)
                    Gratia.Send(gse)
                    srecord_date = srecord['date']
            vo_name = srecord['storage_group']
            DebugPrint(4, "Sending SE/SER for VO %s" % vo_name)
            unique_id = "%s:StorageGroup:%s" % (se, vo_name)
            # the name of the se is the vo_name
            # the timestamp is coming from the 'date' value in the record
            gse = self.get_storage_element(unique_id, se, vo_name, parent_id)
            gser = self.get_storage_element_record(unique_id)

            self.input_to_gsrs(srecord, gse, gser)

            # To print the records being sent
            #gse.Print()
            #gser.Print()

            Gratia.Send(gse)
            Gratia.Send(gser)

        if srecord_date is not None:
            DebugPrintLevel(4, "Sending the parent StorageElement (%s/%s/%s)" % (se, parent_name, srecord_date))
            timestamp = timeutil.format_datetime(timeutil.datetime_to_utc(srecord_date))
            gse = self.get_storage_element(parent_id, se, parent_name, timestamp)
            Gratia.Send(gse)


if __name__ == "__main__":
    EnstoreStorageProbe().main()

